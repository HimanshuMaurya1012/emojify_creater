{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bfe3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724224d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'train'\n",
    "val_dir = 'test'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a2213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Building the convolution neural network architecture:\n",
    "\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8ca41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "C:\\Users\\HIMANSHU\\AppData\\Local\\Temp\\ipykernel_13068\\836729098.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 289s 641ms/step - loss: 1.7970 - accuracy: 0.2618 - val_loss: 1.6840 - val_accuracy: 0.3555\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 188s 420ms/step - loss: 1.6188 - accuracy: 0.3725 - val_loss: 1.5278 - val_accuracy: 0.4237\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 189s 421ms/step - loss: 1.5215 - accuracy: 0.4130 - val_loss: 1.4570 - val_accuracy: 0.4403\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 189s 422ms/step - loss: 1.4545 - accuracy: 0.4434 - val_loss: 1.3983 - val_accuracy: 0.4674\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 205s 457ms/step - loss: 1.3975 - accuracy: 0.4673 - val_loss: 1.3510 - val_accuracy: 0.4867\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 206s 460ms/step - loss: 1.3468 - accuracy: 0.4876 - val_loss: 1.3102 - val_accuracy: 0.5080\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 207s 462ms/step - loss: 1.3038 - accuracy: 0.5053 - val_loss: 1.2885 - val_accuracy: 0.5116\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 215s 479ms/step - loss: 1.2610 - accuracy: 0.5235 - val_loss: 1.2343 - val_accuracy: 0.5303\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 218s 487ms/step - loss: 1.2259 - accuracy: 0.5381 - val_loss: 1.2268 - val_accuracy: 0.5294\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 214s 477ms/step - loss: 1.1980 - accuracy: 0.5488 - val_loss: 1.1989 - val_accuracy: 0.5417\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 223s 497ms/step - loss: 1.1677 - accuracy: 0.5628 - val_loss: 1.1688 - val_accuracy: 0.5557\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 237s 530ms/step - loss: 1.1433 - accuracy: 0.5717 - val_loss: 1.1722 - val_accuracy: 0.5550\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 227s 506ms/step - loss: 1.1129 - accuracy: 0.5835 - val_loss: 1.1478 - val_accuracy: 0.5632\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 219s 489ms/step - loss: 1.0899 - accuracy: 0.5918 - val_loss: 1.1312 - val_accuracy: 0.5728\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 214s 478ms/step - loss: 1.0671 - accuracy: 0.6032 - val_loss: 1.1267 - val_accuracy: 0.5741\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 222s 495ms/step - loss: 1.0471 - accuracy: 0.6088 - val_loss: 1.1022 - val_accuracy: 0.5833\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 219s 489ms/step - loss: 1.0218 - accuracy: 0.6202 - val_loss: 1.0997 - val_accuracy: 0.5869\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 220s 490ms/step - loss: 0.9982 - accuracy: 0.6283 - val_loss: 1.0877 - val_accuracy: 0.5887\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 244s 544ms/step - loss: 0.9760 - accuracy: 0.6384 - val_loss: 1.0863 - val_accuracy: 0.5965\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 238s 532ms/step - loss: 0.9569 - accuracy: 0.6460 - val_loss: 1.0752 - val_accuracy: 0.5971\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 237s 528ms/step - loss: 0.9314 - accuracy: 0.6560 - val_loss: 1.0672 - val_accuracy: 0.5992\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 246s 549ms/step - loss: 0.9088 - accuracy: 0.6660 - val_loss: 1.0689 - val_accuracy: 0.6003\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 235s 525ms/step - loss: 0.8898 - accuracy: 0.6722 - val_loss: 1.0752 - val_accuracy: 0.6039\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 241s 538ms/step - loss: 0.8662 - accuracy: 0.6807 - val_loss: 1.0664 - val_accuracy: 0.6069\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 219s 490ms/step - loss: 0.8451 - accuracy: 0.6891 - val_loss: 1.0638 - val_accuracy: 0.6081\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 223s 498ms/step - loss: 0.8207 - accuracy: 0.7012 - val_loss: 1.0557 - val_accuracy: 0.6092\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 221s 493ms/step - loss: 0.8034 - accuracy: 0.7060 - val_loss: 1.0510 - val_accuracy: 0.6115\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 227s 506ms/step - loss: 0.7790 - accuracy: 0.7132 - val_loss: 1.0588 - val_accuracy: 0.6105\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 220s 491ms/step - loss: 0.7520 - accuracy: 0.7252 - val_loss: 1.0573 - val_accuracy: 0.6144\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 217s 484ms/step - loss: 0.7329 - accuracy: 0.7349 - val_loss: 1.0589 - val_accuracy: 0.6173\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 222s 495ms/step - loss: 0.7116 - accuracy: 0.7414 - val_loss: 1.0754 - val_accuracy: 0.6117\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 225s 503ms/step - loss: 0.6913 - accuracy: 0.7490 - val_loss: 1.0798 - val_accuracy: 0.6145\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 233s 520ms/step - loss: 0.6715 - accuracy: 0.7550 - val_loss: 1.0874 - val_accuracy: 0.6137\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 226s 504ms/step - loss: 0.6556 - accuracy: 0.7629 - val_loss: 1.0725 - val_accuracy: 0.6152\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 209s 466ms/step - loss: 0.6331 - accuracy: 0.7709 - val_loss: 1.0773 - val_accuracy: 0.6176\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 210s 468ms/step - loss: 0.6124 - accuracy: 0.7742 - val_loss: 1.0816 - val_accuracy: 0.6189\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 214s 478ms/step - loss: 0.5861 - accuracy: 0.7881 - val_loss: 1.0953 - val_accuracy: 0.6217\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 223s 497ms/step - loss: 0.5696 - accuracy: 0.7918 - val_loss: 1.0899 - val_accuracy: 0.6235\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 229s 510ms/step - loss: 0.5577 - accuracy: 0.7970 - val_loss: 1.0962 - val_accuracy: 0.6235\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 213s 475ms/step - loss: 0.5399 - accuracy: 0.8042 - val_loss: 1.1058 - val_accuracy: 0.6272\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 222s 496ms/step - loss: 0.5243 - accuracy: 0.8071 - val_loss: 1.0943 - val_accuracy: 0.6228\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 224s 500ms/step - loss: 0.5104 - accuracy: 0.8137 - val_loss: 1.1125 - val_accuracy: 0.6211\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 220s 492ms/step - loss: 0.4924 - accuracy: 0.8187 - val_loss: 1.1155 - val_accuracy: 0.6253\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 221s 494ms/step - loss: 0.4727 - accuracy: 0.8275 - val_loss: 1.1398 - val_accuracy: 0.6223\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 213s 476ms/step - loss: 0.4543 - accuracy: 0.8353 - val_loss: 1.1461 - val_accuracy: 0.6235\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 225s 502ms/step - loss: 0.4422 - accuracy: 0.8420 - val_loss: 1.1544 - val_accuracy: 0.6236\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 224s 499ms/step - loss: 0.4352 - accuracy: 0.8444 - val_loss: 1.1731 - val_accuracy: 0.6215\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 218s 486ms/step - loss: 0.4106 - accuracy: 0.8530 - val_loss: 1.1988 - val_accuracy: 0.6244\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 234s 523ms/step - loss: 0.3998 - accuracy: 0.8578 - val_loss: 1.1872 - val_accuracy: 0.6243\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 230s 514ms/step - loss: 0.3920 - accuracy: 0.8602 - val_loss: 1.1871 - val_accuracy: 0.6258\n"
     ]
    }
   ],
   "source": [
    "#  Compiling and training the model:\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)\n",
    "\n",
    "#   Saving the model weights:\n",
    "\n",
    "emotion_model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceeef37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
